{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "The agent implementation can be found in the root directory in the file _agent.py_.\n",
    "\n",
    "The agent is using a standard DQN implementation with an epsilon-greedy policy, fixed target weights, random sampling from a memory buffer, as well as using the following hyperparameters:\n",
    "- BUFFER_SIZE = 1e5  \n",
    "- BATCH_SIZE = 64         \n",
    "- GAMMA = 0.99            \n",
    "- TAU = 1e-3              \n",
    "- LEARNING_RATE = 5e-4  \n",
    "\n",
    "**Edit:** The learning algorithm works as follows:\n",
    "1. Select an action from the agent's epsilon greedy policy.  This means with some decaying-per-episode probability the actions selected will go from completely random to favoring the optimal action.  The optimal action is determined by a function approximation neural network which is described below in the cell labelled Q-Net Model, and this network acts as the value function for a state and outputting the best action determined for that state\n",
    "2. The action is taken in the environment and the next state, reward, and doneness are observed. These, along with the current state and action performed, are saved as a tuple in the agent's \"memory\" queue which will keep the most recent BUFFER_SIZE (100000) of these memories.\n",
    "3. After every 4 timesteps, the memory queue is randomly sampled and trains the Q-Network.  It does this by using a second network that updates at a slower (indicated by TAU) than the actual network we want to use to make decisions.  The target network determines the value of the next states that the state-actions that the memories lead to, then uses this to add (at a reduced value indicated by GAMMA) to the reward of the memory in order to get the value of the current state-action.  The local network then re-evaluates these states and determines the value of the actions that it would take now from those states.  The newly evaluated state to action value by the local network is used to compare against the state-action values (that the target network determined) in the loss function (Mean Squared Error).  The local network updates based on this loss and the learning rate, then the state of the local network updates the target network at an even slower rate (TAU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Net Model\n",
    "The network implementation can be found in the root directory in the file _qnetmodel.py_.\n",
    "\n",
    "The model consists of 3 dense layers with input units for the state size (37), hidden layer of 64 units, and output units for the action size (4).  The input and hidden layers have relu activation functions and the output produces raw values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards plot\n",
    "Below is the plot of total scores per episode and it shows an upwards trend with a slight taper towards the end, however after a little over 500 episodes, the most-recent-100-episode average reached the goal of >=13.\n",
    "<img src=\"files/training_scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Future Improvements\n",
    "I am a big fan of the actor critic model and I think it is a much more robust version of DQN.  If this implementation proved to be insufficient, that would be my next go-to model.  I found that during the trained run there was a lot of useless \"turning around\" that could have gotten \"criticized\" better, leading to a more efficient gathering policy.\n",
    "\n",
    "Additionally, a prioritized memory queue would probably benefit because there are some rarer states that are worth more reward, such as when two bananas are on top of each other.  Having a prioritized memory queue could favor learning from those experiences more, leading the agent to try to give more value to those doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
